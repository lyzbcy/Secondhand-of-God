# 《神之手：最后的防线》机器学习与深度学习技术文档

> 本文档详细介绍项目中应用的机器学习与深度学习技术，适用于实验报告撰写

---

## 1. 项目概述

**项目名称**：神之手：最后的防线 (God Hand: Last Defense)  
**项目类型**：基于手势识别的塔防游戏  
**核心技术**：深度学习手势识别 + 强化学习智能体  

---

## 2. 深度学习技术应用

### 2.1 MediaPipe Hands - 手部追踪模型

#### 2.1.1 技术原理

**MediaPipe Hands** 是Google开发的实时手部追踪解决方案，采用机器学习流水线架构：

```
输入视频流 → 手掌检测 → 手部关键点定位 → 手势分类
```

**模型架构**：
1. **Palm Detection Model**（手掌检测模型）
   - 基于 **SSD (Single Shot Detector)** 轻量级目标检测网络
   - 输入：RGB图像帧
   - 输出：手掌边界框和置信度

2. **Hand Landmark Model**（手部关键点模型）
   - 基于 **CNN (Convolutional Neural Network)**
   - 输出：21个3D手部关键点坐标 (x, y, z)
   - 关键点包括：手腕、掌根、5指各4个关节

#### 2.1.2 项目中的应用

**配置参数**：
```javascript
// hand-tracker.js 第62-68行
this.hands.setOptions({
    maxNumHands: 4,              // 最大检测手数（双人模式）
    modelComplexity: 1,           // 模型复杂度（0/1/2）
    minDetectionConfidence: 0.7,  // 检测置信度阈值
    minTrackingConfidence: 0.5    // 追踪置信度阈值
});
```

**模型复杂度说明**：
- `modelComplexity: 0` - Lite模型，速度最快，精度较低
- `modelComplexity: 1` - Full模型，平衡速度和精度 ✅ **本项目采用**
- `modelComplexity: 2` - 高精度模型，速度较慢

**实时推理流程**：
```javascript
// hand-tracker.js 第116-119行
const camera = new Camera(this.videoElement, {
    onFrame: async () => {
        await this.hands.send({ image: this.videoElement });
    }
});
```

每一帧视频都会：
1. 通过 **Palm Detection** 检测手掌位置
2. 使用 **Hand Landmark Model** 提取21个关键点
3. 返回归一化坐标 (0-1范围) 和3D深度信息

#### 2.1.3 多手追踪机制

本项目支持**双人模式**，需同时追踪4只手：

```javascript
// hand-tracker.js 第126-195行
onResults(results) {
    if (results.multiHandLandmarks && results.multiHandedness) {
        for (let i = 0; i < results.multiHandLandmarks.length; i++) {
            const landmarks = results.multiHandLandmarks[i];
            const handedness = results.multiHandedness[i];
            
            // 计算手掌中心用于玩家识别
            const normalizedX = landmarks[9].x;
            
            // 基于X坐标区分玩家（屏幕空间分区）
            const playerId = normalizedX < 0.5 ? 1 : 2;
        }
    }
}
```

**玩家识别算法**：
- 利用手掌根部关键点（索引9）的X坐标
- 屏幕左半部分 (x < 0.5) → 玩家1
- 屏幕右半部分 (x ≥ 0.5) → 玩家2

---

### 2.2 手势识别算法

虽然MediaPipe提供关键点，但手势语义识别需要自定义算法。

#### 2.2.1 特征工程

**手指伸展检测**：
```javascript
// hand-tracker.js 第310-315行
getExtendedFingers(lm) {
    return [4, 8, 12, 16, 20].map((tipIdx, i) => {
        if (i === 0) return Utils.distance(lm[4].x, lm[4].y, lm[2].x, lm[2].y) > 0.05;
        return lm[tipIdx].y < lm[[3, 6, 10, 14, 18][i]].y;
    });
}
```

通过比较**指尖**和**指关节**的Y坐标判断手指是否伸展。

**捏合手势检测**：
```javascript
// hand-tracker.js 第303-307行
isPinching(lm) {
    const thumbToIndex = Utils.distance(lm[4].x, lm[4].y, lm[8].x, lm[8].y);
    const thumbToMiddle = Utils.distance(lm[4].x, lm[4].y, lm[12].x, lm[12].y);
    return thumbToIndex < 0.12 || thumbToMiddle < 0.14;
}
```

计算**拇指尖**与**食指尖/中指尖**的欧氏距离。

#### 2.2.2 手势分类器

基于**规则的分类系统**：

| 手势类型 | 特征条件 | 游戏功能 |
|---------|---------|---------|
| **握拳** | 伸展手指 ≤ 1 | 锤击敌人/挖矿 |
| **张开手掌** | 伸展手指 ≥ 4 | 拍击敌人 |
| **捏合** | 拇指-食指距离 < 0.12 | 拖拽建造防御塔 |
| **双手合十** | 双手掌心距离 < 0.15 | 释放大招 |

**动态手势识别**（加入速度特征）：
```javascript
// hand-tracker.js 第278-293行
const velocity = this.getHandVelocity(handLabel);

if (velocity.speed > 100) {
    if (isFist && velocity.speed > 150) {
        this.triggerGesture('punch', handLabel, screenLandmarks);
    }
    else if (!isFist && !isPinching && velocity.speed > 120) {
        this.triggerGesture('chop', handLabel, screenLandmarks);
    }
}
```

结合**手掌移动速度**判断动作意图（挥拳、劈砍）。

---

## 3. 强化学习技术应用

### 3.1 Q-Learning算法实现

项目实现了一个**强化学习智能体**控制敌人AI，采用经典的**Q-Learning**算法。

#### 3.1.1 算法原理

**Q-Learning**是一种无模型(Model-Free)的时序差分(TD)强化学习算法：

$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
$$

其中：
- $Q(s, a)$ - 状态-动作价值函数
- $\alpha$ - 学习率 (learning rate)
- $r$ - 即时奖励 (reward)
- $\gamma$ - 折扣因子 (discount factor)
- $s'$ - 下一状态

#### 3.1.2 状态空间设计

```javascript
// rl-agent.js
encodeState(enemy, game) {
    const crystal = game.crystal;
    const dist = Utils.distance(enemy.x, enemy.y, crystal.x, crystal.y);
    
    // 离散化状态空间
    const distBucket = Math.min(9, Math.floor(dist / 100));
    const hpBucket = Math.min(4, Math.floor(enemy.hp / 50));
    const threatBucket = this.getThreatLevel(enemy, game);
    
    return `${distBucket}-${hpBucket}-${threatBucket}`;
}
```

**状态维度**：
1. **距离桶** (0-9)：敌人到水晶的距离
2. **HP桶** (0-4)：敌人当前血量
3. **威胁等级** (0-2)：周围防御塔数量

总状态空间大小：10 × 5 × 3 = **150个状态**

#### 3.1.3 动作空间

```javascript
this.actions = [
    'direct',   // 直线冲锋
    'left',     // 左侧绕行
    'right',    // 右侧绕行
    'retreat',  // 后退躲避
    'charge'    // 高速冲锋
];
```

**动作执行**：
```javascript
getMovementFromAction(action, enemy, game) {
    const crystal = game.crystal;
    const angle = Utils.angle(enemy.x, enemy.y, crystal.x, crystal.y);
    
    switch(action) {
        case 'direct':
            return { dx: Math.cos(angle), dy: Math.sin(angle), speedMultiplier: 1.0 };
        case 'left':
            return { dx: Math.cos(angle - 0.5), dy: Math.sin(angle - 0.5), speedMultiplier: 0.9 };
        // ...
    }
}
```

#### 3.1.4 奖励函数设计

```javascript
calculateReward(enemy, game, event = {}) {
    let reward = 0;
    
    // 正奖励
    if (event.type === 'attackCrystal') {
        reward = 100;  // 成功攻击水晶
    }
    
    // 负奖励  
    if (event.type === 'killed') {
        reward = -50;  // 被杀死
    }
    if (event.type === 'damaged') {
        reward = -10;  // 受到伤害
    }
    
    // 距离奖励（引导向水晶移动）
    const currentDist = Utils.distance(enemy.x, enemy.y, 
                                      game.crystal.x, game.crystal.y);
    if (enemy.prevDist != null) {
        reward += (enemy.prevDist - currentDist) * 0.1;
    }
    
    return reward;
}
```

**奖励策略**：
- ✅ **稀疏奖励**：重要事件（攻击成功、死亡）
- ✅ **密集奖励**：持续的距离变化反馈
- ✅ **负奖励惩罚**：避免被防御塔击杀

#### 3.1.5 探索-利用平衡

**Epsilon-Greedy策略**：
```javascript
chooseAction(state) {
    if (Math.random() < this.epsilon) {
        // 探索：随机选择动作
        return this.actions[Math.floor(Math.random() * this.actions.length)];
    } else {
        // 利用：选择Q值最高的动作
        return this.getBestAction(state);
    }
}
```

**Epsilon衰减**：
```javascript
this.epsilon = Math.max(0.1, this.epsilon * 0.995);  // 每回合衰减
```

从**高探索**（ε=1.0）逐渐过渡到**高利用**（ε=0.1）。

---

## 4. 模型训练与优化

### 4.1 在线学习

Q-Learning采用**在线学习**方式，每次敌人行动都立即更新Q表：

```javascript
learn(state, action, reward, nextState, done) {
    if (!this.qTable[state]) {
        this.qTable[state] = {};
        this.actions.forEach(a => this.qTable[state][a] = 0);
    }
    
    const currentQ = this.qTable[state][action];
    const maxNextQ = done ? 0 : this.getMaxQ(nextState);
    
    // Q-Learning更新公式
    const newQ = currentQ + this.alpha * (reward + this.gamma * maxNextQ - currentQ);
    this.qTable[state][action] = newQ;
}
```

### 4.2 超参数配置

| 参数 | 值 | 说明 |
|------|-----|------|
| **学习率** α | 0.1 | 控制更新步长 |
| **折扣因子** γ | 0.9 | 未来奖励的重要性 |
| **初始探索率** ε | 1.0 | 开始时完全随机探索 |
| **最小探索率** | 0.1 | 保留10%探索概率 |
| **衰减率** | 0.995 | 每回合探索率衰减 |

### 4.3 模型持久化

```javascript
saveModel() {
    const modelData = {
        qTable: this.qTable,
        epsilon: this.epsilon,
        stats: this.stats
    };
    localStorage.setItem('rl_model', JSON.stringify(modelData));
}

loadModel() {
    const saved = localStorage.getItem('rl_model');
    if (saved) {
        const data = JSON.parse(saved);
        this.qTable = data.qTable;
        this.epsilon = data.epsilon;
    }
}
```

Q表存储在**浏览器LocalStorage**，支持跨会话学习。

---

## 5. 性能优化

### 5.1 MediaPipe优化

**模型选择**：
- 使用 `modelComplexity: 1` 平衡精度和速度
- 在中等配置设备上维持 **30+ FPS**

**置信度阈值调优**：
```javascript
minDetectionConfidence: 0.7   // 降低误检
minTrackingConfidence: 0.5    // 提高追踪稳定性
```

### 5.2 RL训练加速

**状态空间压缩**：
- 使用离散化桶而非连续值
- 150个状态 vs 理论上的无限状态

**经验回放**（未实现，可扩展）：
- 可添加 Experience Replay Buffer
- 打破时间相关性，提高样本利用率

---

## 6. 技术创新点

### 6.1 多手追踪玩家识别

**创新**：基于屏幕空间分区自动识别玩家身份
- 无需额外的人脸识别或ID输入
- 适用于协作游戏场景

### 6.2 在线强化学习

**创新**：敌人AI通过与玩家对战实时学习
- 游戏难度动态调整
- 每个玩家的游戏体验独特

### 6.3 轻量级部署

**创新**：纯前端实现，无需服务器
- MediaPipe模型通过CDN加载
- RL模型存储在浏览器本地

---

## 7. 未来改进方向

### 7.1 深度强化学习

当前使用的Q-Learning适合小规模状态空间，可以升级为：
- **DQN (Deep Q-Network)**：使用神经网络逼近Q函数
- **PPO (Proximal Policy Optimization)**：更稳定的策略梯度方法

### 7.2 迁移学习

MediaPipe模型是预训练的，可以考虑：
- **Fine-tuning**：在特定手势数据集上微调
- **自定义手势**：训练识别游戏特有手势

### 7.3 多模态融合

结合多种输入：
- 手势 + 语音指令
- 手势 + 眼动追踪

---

## 8. 总结

本项目成功应用了以下机器学习与深度学习技术：

| 技术类别 | 具体技术 | 应用场景 |
|---------|---------|---------|
| **深度学习** | MediaPipe Hands CNN | 实时手部追踪和关键点检测 |
| **计算机视觉** | 手势识别算法 | 将关键点转换为游戏指令 |
| **强化学习** | Q-Learning | 敌人AI智能决策 |
| **在线学习** | 增量更新Q表 | 实时适应玩家策略 |

**技术亮点**：
✅ 实时性能优异（30+ FPS）  
✅ 支持多人协作（4手追踪）  
✅ AI自适应学习能力  
✅ 纯前端轻量级部署  

**代码实现**：
- [hand-tracker.js](../js/hand-tracker.js) - MediaPipe集成和手势识别
- [rl-agent.js](../js/rl-agent.js) - Q-Learning实现
- [enemy-system.js](../js/enemy-system.js) - RL控制的敌人行为

---

*文档版本：1.0 | 创建日期：2025-12-14*
